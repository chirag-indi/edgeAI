{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Edge AI Trust Project AI at the edge has the opportunity to embody greater forms of trust by utilizing the local VPN. Taking advantage of the VPN to share data, replicate findings, or expose behavior could have the effect of increased overall application resiliency as well as contributing to accountability and explainability. We are standing up a small Nvidia Jetson Nano network using EdgeVPN. The applications we hope to deploy are federated learning and redundant learning models. The objective is to deploy co-resident tools that in effect \"sense and respond\" for enhanced accountability and explainability.","title":"Home"},{"location":"#edge-ai-trust-project","text":"AI at the edge has the opportunity to embody greater forms of trust by utilizing the local VPN. Taking advantage of the VPN to share data, replicate findings, or expose behavior could have the effect of increased overall application resiliency as well as contributing to accountability and explainability. We are standing up a small Nvidia Jetson Nano network using EdgeVPN. The applications we hope to deploy are federated learning and redundant learning models. The objective is to deploy co-resident tools that in effect \"sense and respond\" for enhanced accountability and explainability.","title":"Edge AI Trust Project"},{"location":"12-May-2021/","text":"12-May-2021 Wednesday May, 12 2021 12:00 ET Future Work Create a documentation website Create a visualization with the 3 Jetson Nano nodes. Confirm that nodes are communicating with each other when the VPN is active.","title":"12-May-2021"},{"location":"12-May-2021/#12-may-2021","text":"Wednesday May, 12 2021 12:00 ET","title":"12-May-2021"},{"location":"12-May-2021/#future-work","text":"Create a documentation website Create a visualization with the 3 Jetson Nano nodes. Confirm that nodes are communicating with each other when the VPN is active.","title":"Future Work"},{"location":"deployFlask/","text":"Deploying a simple Flask application The basic functionality of the application is to accept a request initiated in any of the pods and take a random walk through the remaining active pods dynamically fetching and using the flannel IP associated with the each of the pods. The code is currently in the branch flask-application . Building the Multiarch docker Image A multiarchitecture Docker image facilitates the use of edge IoT devices to run the Flask server. This is done using Docker Buildx which is a CLI plugin that extends the docker command and is designed to work well for building for multiple platforms and not only for the architecture and operating system that the user invoking the build happens to run. Create a new builder instance and update the current configuration $ docker buildx create --name mybuilder $ docker buildx use mybuilder $ docker buildx inspect --bootstrap Install the multiarch emulator docker support $ docker run --privileged --rm tonistiigi/binfmt --install all Change directory into the folder with the Flask application and build and push the Dockerfile with support to amd64 and arm64. $ cd /path/to/folder $ docker buildx build --platform linux/amd64,linux/arm64,linux/arm/v7 -t chiragindi/d2iedgeai:latest --push . Creating the deployments and services in Kubernetes The following yaml file is saved into a file on the kuberenets cluster. apiVersion: apps/v1 kind: Deployment metadata: labels: app: d2iedgeai1 name: d2iedgeai1 spec: replicas: 1 selector: matchLabels: app: d2iedgeai1 template: metadata: labels: app: d2iedgeai1 spec: containers: - image: chiragindi/d2iedgeai name: d2iedgeai1 imagePullPolicy: Always ports: - containerPort: 12000 env: - name: DEBUG value: \"True\" - name: NODE_IP valueFrom: fieldRef: fieldPath: status.podIP restartPolicy: Always nodeSelector: nodenumber: one --- apiVersion: v1 kind: Service metadata: labels: app: d2iedgeai1 name: d2iedgeai1 spec: type: NodePort selector: app: d2iedgeai1 ports: - port: 12000 targetPort: 30001 --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: d2iedgeai2 name: d2iedgeai2 spec: replicas: 1 selector: matchLabels: app: d2iedgeai2 template: metadata: labels: app: d2iedgeai2 spec: containers: - image: chiragindi/d2iedgeai name: d2iedgeai2 imagePullPolicy: Always ports: - containerPort: 12000 env: - name: DEBUG value: \"True\" - name: NODE_IP valueFrom: fieldRef: fieldPath: status.podIP restartPolicy: Always nodeSelector: nodenumber: two --- apiVersion: v1 kind: Service metadata: labels: app: d2iedgeai2 name: d2iedgeai2 spec: type: NodePort selector: app: d2iedgeai2 ports: - port: 12000 targetPort: 30002 --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: d2iedgeai3 name: d2iedgeai3 spec: replicas: 1 selector: matchLabels: app: d2iedgeai3 template: metadata: labels: app: d2iedgeai3 spec: containers: - image: chiragindi/d2iedgeai name: d2iedgeai3 imagePullPolicy: Always ports: - containerPort: 12000 env: - name: DEBUG value: \"True\" - name: NODE_IP valueFrom: fieldRef: fieldPath: status.podIP restartPolicy: Always nodeSelector: nodenumber: three --- apiVersion: v1 kind: Service metadata: labels: app: d2iedgeai3 name: d2iedgeai3 spec: type: NodePort selector: app: d2iedgeai3 ports: - port: 12000 targetPort: 30003 --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: fabric8-rbac subjects: - kind: ServiceAccount # Reference to upper's `metadata.name` name: default # Reference to upper's `metadata.namespace` namespace: default roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io The following command is then run to create the above mentioned resources. The RBAC is required for the kubernetes Python API to work. # kubectl apply -f config.yaml","title":"Deploying a Flask application"},{"location":"deployFlask/#deploying-a-simple-flask-application","text":"The basic functionality of the application is to accept a request initiated in any of the pods and take a random walk through the remaining active pods dynamically fetching and using the flannel IP associated with the each of the pods. The code is currently in the branch flask-application .","title":"Deploying a simple Flask application"},{"location":"deployFlask/#building-the-multiarch-docker-image","text":"A multiarchitecture Docker image facilitates the use of edge IoT devices to run the Flask server. This is done using Docker Buildx which is a CLI plugin that extends the docker command and is designed to work well for building for multiple platforms and not only for the architecture and operating system that the user invoking the build happens to run.","title":"Building the Multiarch docker Image"},{"location":"deployFlask/#create-a-new-builder-instance-and-update-the-current-configuration","text":"$ docker buildx create --name mybuilder $ docker buildx use mybuilder $ docker buildx inspect --bootstrap","title":"Create a new builder instance and update the current configuration"},{"location":"deployFlask/#install-the-multiarch-emulator-docker-support","text":"$ docker run --privileged --rm tonistiigi/binfmt --install all Change directory into the folder with the Flask application and build and push the Dockerfile with support to amd64 and arm64. $ cd /path/to/folder $ docker buildx build --platform linux/amd64,linux/arm64,linux/arm/v7 -t chiragindi/d2iedgeai:latest --push .","title":"Install the multiarch emulator docker support"},{"location":"deployFlask/#creating-the-deployments-and-services-in-kubernetes","text":"The following yaml file is saved into a file on the kuberenets cluster. apiVersion: apps/v1 kind: Deployment metadata: labels: app: d2iedgeai1 name: d2iedgeai1 spec: replicas: 1 selector: matchLabels: app: d2iedgeai1 template: metadata: labels: app: d2iedgeai1 spec: containers: - image: chiragindi/d2iedgeai name: d2iedgeai1 imagePullPolicy: Always ports: - containerPort: 12000 env: - name: DEBUG value: \"True\" - name: NODE_IP valueFrom: fieldRef: fieldPath: status.podIP restartPolicy: Always nodeSelector: nodenumber: one --- apiVersion: v1 kind: Service metadata: labels: app: d2iedgeai1 name: d2iedgeai1 spec: type: NodePort selector: app: d2iedgeai1 ports: - port: 12000 targetPort: 30001 --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: d2iedgeai2 name: d2iedgeai2 spec: replicas: 1 selector: matchLabels: app: d2iedgeai2 template: metadata: labels: app: d2iedgeai2 spec: containers: - image: chiragindi/d2iedgeai name: d2iedgeai2 imagePullPolicy: Always ports: - containerPort: 12000 env: - name: DEBUG value: \"True\" - name: NODE_IP valueFrom: fieldRef: fieldPath: status.podIP restartPolicy: Always nodeSelector: nodenumber: two --- apiVersion: v1 kind: Service metadata: labels: app: d2iedgeai2 name: d2iedgeai2 spec: type: NodePort selector: app: d2iedgeai2 ports: - port: 12000 targetPort: 30002 --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: d2iedgeai3 name: d2iedgeai3 spec: replicas: 1 selector: matchLabels: app: d2iedgeai3 template: metadata: labels: app: d2iedgeai3 spec: containers: - image: chiragindi/d2iedgeai name: d2iedgeai3 imagePullPolicy: Always ports: - containerPort: 12000 env: - name: DEBUG value: \"True\" - name: NODE_IP valueFrom: fieldRef: fieldPath: status.podIP restartPolicy: Always nodeSelector: nodenumber: three --- apiVersion: v1 kind: Service metadata: labels: app: d2iedgeai3 name: d2iedgeai3 spec: type: NodePort selector: app: d2iedgeai3 ports: - port: 12000 targetPort: 30003 --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: fabric8-rbac subjects: - kind: ServiceAccount # Reference to upper's `metadata.name` name: default # Reference to upper's `metadata.namespace` namespace: default roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io The following command is then run to create the above mentioned resources. The RBAC is required for the kubernetes Python API to work. # kubectl apply -f config.yaml","title":"Creating the deployments and services in Kubernetes"},{"location":"install/","text":"Install the VPN You need to add the evio repository to your node - this step needs only be done once for a host: sudo bash # echo \"deb [trusted=yes] https://apt.fury.io/evio/ /\" > /etc/apt/sources.list.d/fury.list # apt update To install the package: # apt install evio If the installation fails due to the libffi-dev dependence, you might need to add that manually: # apt-get install libffi-dev # apt install evio Edit Configuration File After installation, but before starting, configure your node by editing /etc/opt/evio/config.json. The easiest way to get started with a working configuration is to request a trial account. You can also use the template from this page and add XMPP credentials, setting the IP address, and applying other configurations as needed Run Service and disable multicast Replace appbrXXXXX with the name of your Evio bridge in the command below to disable multicast: sudo systemctl start evio sudo ip link set appbrXXXXX multicast off Additionally, use systemctl to start/stop/restart/status evio. Dependencies The installer has dependencies on, and will install python3 (>=3.8), python3-dev (>=3.8), python3-pip, iproute2, bridge-utils. Related Files and Directories By default, the following files and directories are created: /opt/evio/tincan /opt/evio/controller/ /etc/opt/evio/config.json Disabling or removing the software To disable Start on Boot: sudo systemctl disable evio To remove the package: sudo apt remove -y evio","title":"Installing EdgeVPN"},{"location":"install/#install-the-vpn","text":"You need to add the evio repository to your node - this step needs only be done once for a host: sudo bash # echo \"deb [trusted=yes] https://apt.fury.io/evio/ /\" > /etc/apt/sources.list.d/fury.list # apt update To install the package: # apt install evio If the installation fails due to the libffi-dev dependence, you might need to add that manually: # apt-get install libffi-dev # apt install evio","title":"Install the VPN"},{"location":"install/#edit-configuration-file","text":"After installation, but before starting, configure your node by editing /etc/opt/evio/config.json. The easiest way to get started with a working configuration is to request a trial account. You can also use the template from this page and add XMPP credentials, setting the IP address, and applying other configurations as needed","title":"Edit Configuration File"},{"location":"install/#run-service-and-disable-multicast","text":"Replace appbrXXXXX with the name of your Evio bridge in the command below to disable multicast: sudo systemctl start evio sudo ip link set appbrXXXXX multicast off Additionally, use systemctl to start/stop/restart/status evio.","title":"Run Service and disable multicast"},{"location":"install/#dependencies","text":"The installer has dependencies on, and will install python3 (>=3.8), python3-dev (>=3.8), python3-pip, iproute2, bridge-utils.","title":"Dependencies"},{"location":"install/#related-files-and-directories","text":"By default, the following files and directories are created: /opt/evio/tincan /opt/evio/controller/ /etc/opt/evio/config.json","title":"Related Files and Directories"},{"location":"install/#disabling-or-removing-the-software","text":"To disable Start on Boot: sudo systemctl disable evio To remove the package: sudo apt remove -y evio","title":"Disabling or removing the software"},{"location":"kernel/","text":"Building a custom Kernel The stock kernel distributed with Ubuntu 18.04 for Jetson devices does not come with proper dependences to support the Open vSwitch virtual switch that EdgeVPN.io builds upon. While there are different ways to create a workflow to run Evio-enabled Jetson devices, such as cross-compilation and flashing a custom image, a simpler approach that works for development and testing is to boot up the Jetson device with the stock kernel/image, and build a new kernel that includes the proper dependences. NOTE: the process below has been tested on Jetson nano development kit devices. We have not tested other models of Jetson devices. Building the kernel Once you have booted the device into the stock kernel, you need to follow the approach outlined in the nVidia documentation to build a custom kernel You will need an nVidia developer account to be able to access kernel source files. Ensure your OS has proper dependences installed sudo apt update sudo apt install git-core sudo apt install build-essential bc Download and unpack kernel sources You can login with your nVidia dev account and browse the downloads page, then select the latest \u201cL4T Jetson Driver Package\u201d to download Copy the downloaded archive to your home directory on the nano, and expand it with: tar xf Tegra210_Linux_*.tbz2 Sync gode with git repo cd Linux_for_Tegra ./source_sync.sh When prompted, enter the git tag you want sync to, e.g. tegra-l4t-r32.5 You may be asked multiple times. Create baseline kernel build config file Select a directory for your build, e.g. /home/username/build-ovs TEGRA_KERNEL_OUT=/home/username/build-ovs cd ~/Linux_for_Tegra/sources/kernel/kernel-4.9 mkdir -p $TEGRA_KERNEL_OUT make ARCH=arm64 O=$TEGRA_KERNEL_OUT tegra_defconfig Edit .config to enable Open vSwitch modules Now, you need to edit the .config file (with kernel build configuration parameters) to add Open vSwitch and GRE support as modules vi $TEGRA_KERNEL_OUT/.config Uncomment/add the following config entries: CONFIG_NET_IPGRE=m CONFIG_NET_IPGRE_DEMUX=m CONFIG_OPENVSWITCH=m CONFIG_OPENVSWITCH_GRE=m CONFIG_OPENVSWITCH_VXLAN=m Build the kernel make ARCH=arm64 O=$TEGRA_KERNEL_OUT -j4 You will be prompted about these (and perhaps other) options; enter your choices (N=no, m=module) manually: CONFIG_NET_MPLS_GSO=m CONFIG_MPLS_ROUTING=n CONFIG_PPTP=n This will take more than an hour to complete. Copy kernel image and modules Once the kernel is compiled, you need to copy the kernel image to /boot: sudo cp $TEGRA_KERNEL_OUT/arch/arm64/boot/Image /boot/Image-ovs You also need to install and copy kernel modules: sudo make ARCH=arm64 O=$TEGRA_KERNEL_OUT modules_install \\ INSTALL_MOD_PATH=~/Linux_for_Tegra/rootfs/ pushd ~/Linux_for_Tegra/rootfs sudo tar --owner root --group root -cjf kernel-ovs-modules.tbz2 lib/modules popd pushd / sudo tar -xf ~/Linux_for_Tegra/rootfs/kernel-ovs-modules.tbz2 popd Edit boot config file Edit /boot/extlinux/extlinux.conf as follows to boot from the new kernel you just built as the primary option: TIMEOUT 30 DEFAULT primary MENU TITLE L4T boot options LABEL primary MENU LABEL primary kernel LINUX /boot/Image-ovs INITRD /boot/initrd APPEND ${cbootargs} quiet root=/dev/mmcblk0p1 rw rootwait rootfstype=ext4 console=ttyS0,115200n8 console=tty0 fbcon=map:0 net.ifnames=0 # When testing a custom kernel, it is recommended that you create a backup of # the original kernel and add a new entry to this file so that the device can # fallback to the original kernel. To do this: # # 1, Make a backup of the original kernel # sudo cp /boot/Image /boot/Image.backup # # 2, Copy your custom kernel into /boot/Image # # 3, Uncomment below menu setting lines for the original kernel # # 4, Reboot LABEL backup MENU LABEL backup kernel LINUX /boot/Image INITRD /boot/initrd APPEND ${cbootargs} quiet root=/dev/mmcblk0p1 rw rootwait rootfstype=ext4 console=ttyS0,115200n8 console=tty0 fbcon=map:0 net.ifnames=0","title":"Building a custom kernel"},{"location":"kernel/#building-a-custom-kernel","text":"The stock kernel distributed with Ubuntu 18.04 for Jetson devices does not come with proper dependences to support the Open vSwitch virtual switch that EdgeVPN.io builds upon. While there are different ways to create a workflow to run Evio-enabled Jetson devices, such as cross-compilation and flashing a custom image, a simpler approach that works for development and testing is to boot up the Jetson device with the stock kernel/image, and build a new kernel that includes the proper dependences. NOTE: the process below has been tested on Jetson nano development kit devices. We have not tested other models of Jetson devices.","title":"Building a custom Kernel"},{"location":"kernel/#building-the-kernel","text":"Once you have booted the device into the stock kernel, you need to follow the approach outlined in the nVidia documentation to build a custom kernel You will need an nVidia developer account to be able to access kernel source files.","title":"Building the kernel"},{"location":"kernel/#ensure-your-os-has-proper-dependences-installed","text":"sudo apt update sudo apt install git-core sudo apt install build-essential bc","title":"Ensure your OS has proper dependences installed"},{"location":"kernel/#download-and-unpack-kernel-sources","text":"You can login with your nVidia dev account and browse the downloads page, then select the latest \u201cL4T Jetson Driver Package\u201d to download Copy the downloaded archive to your home directory on the nano, and expand it with: tar xf Tegra210_Linux_*.tbz2","title":"Download and unpack kernel sources"},{"location":"kernel/#sync-gode-with-git-repo","text":"cd Linux_for_Tegra ./source_sync.sh When prompted, enter the git tag you want sync to, e.g. tegra-l4t-r32.5 You may be asked multiple times.","title":"Sync gode with git repo"},{"location":"kernel/#create-baseline-kernel-build-config-file","text":"Select a directory for your build, e.g. /home/username/build-ovs TEGRA_KERNEL_OUT=/home/username/build-ovs cd ~/Linux_for_Tegra/sources/kernel/kernel-4.9 mkdir -p $TEGRA_KERNEL_OUT make ARCH=arm64 O=$TEGRA_KERNEL_OUT tegra_defconfig","title":"Create baseline kernel build config file"},{"location":"kernel/#edit-config-to-enable-open-vswitch-modules","text":"Now, you need to edit the .config file (with kernel build configuration parameters) to add Open vSwitch and GRE support as modules vi $TEGRA_KERNEL_OUT/.config Uncomment/add the following config entries: CONFIG_NET_IPGRE=m CONFIG_NET_IPGRE_DEMUX=m CONFIG_OPENVSWITCH=m CONFIG_OPENVSWITCH_GRE=m CONFIG_OPENVSWITCH_VXLAN=m","title":"Edit .config to enable Open vSwitch modules"},{"location":"kernel/#build-the-kernel","text":"make ARCH=arm64 O=$TEGRA_KERNEL_OUT -j4 You will be prompted about these (and perhaps other) options; enter your choices (N=no, m=module) manually: CONFIG_NET_MPLS_GSO=m CONFIG_MPLS_ROUTING=n CONFIG_PPTP=n This will take more than an hour to complete.","title":"Build the kernel"},{"location":"kernel/#copy-kernel-image-and-modules","text":"Once the kernel is compiled, you need to copy the kernel image to /boot: sudo cp $TEGRA_KERNEL_OUT/arch/arm64/boot/Image /boot/Image-ovs You also need to install and copy kernel modules: sudo make ARCH=arm64 O=$TEGRA_KERNEL_OUT modules_install \\ INSTALL_MOD_PATH=~/Linux_for_Tegra/rootfs/ pushd ~/Linux_for_Tegra/rootfs sudo tar --owner root --group root -cjf kernel-ovs-modules.tbz2 lib/modules popd pushd / sudo tar -xf ~/Linux_for_Tegra/rootfs/kernel-ovs-modules.tbz2 popd","title":"Copy kernel image and modules"},{"location":"kernel/#edit-boot-config-file","text":"Edit /boot/extlinux/extlinux.conf as follows to boot from the new kernel you just built as the primary option: TIMEOUT 30 DEFAULT primary MENU TITLE L4T boot options LABEL primary MENU LABEL primary kernel LINUX /boot/Image-ovs INITRD /boot/initrd APPEND ${cbootargs} quiet root=/dev/mmcblk0p1 rw rootwait rootfstype=ext4 console=ttyS0,115200n8 console=tty0 fbcon=map:0 net.ifnames=0 # When testing a custom kernel, it is recommended that you create a backup of # the original kernel and add a new entry to this file so that the device can # fallback to the original kernel. To do this: # # 1, Make a backup of the original kernel # sudo cp /boot/Image /boot/Image.backup # # 2, Copy your custom kernel into /boot/Image # # 3, Uncomment below menu setting lines for the original kernel # # 4, Reboot LABEL backup MENU LABEL backup kernel LINUX /boot/Image INITRD /boot/initrd APPEND ${cbootargs} quiet root=/dev/mmcblk0p1 rw rootwait rootfstype=ext4 console=ttyS0,115200n8 console=tty0 fbcon=map:0 net.ifnames=0","title":"Edit boot config file"},{"location":"prereq/","text":"Prerequisites of the Server/Master Node The Kubernetes cluster is running inside a docker container on an AMD64/x86_64 system. Sysbox Runtime , a open-source container runtime is used enable the docker container to esentially act as an VM enabling us to run a Kubernetes Cluster inside the container. Installing docker with apt-get #!/bin/sh # https://docs.docker.com/engine/installation/linux/ubuntu/#install-using-the-repository sudo apt-get update && sudo apt-get install -y apt-transport-https ca-certificates curl software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-key fingerprint 0EBFCD88 | grep docker@docker.com || exit 1 sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" sudo apt-get update sudo apt-get install -y docker-ce sudo docker run --rm hello-world Installing using the latest Docker release $ curl -fsSL https://get.docker.com/ | sh Finishing installation Optional - add $USER to docker group (make sure to log out and back in after) $ sudo groupadd docker $ sudo usermod -aG docker $USER $ sudo systemctl restart docker Logout and login so that group changes are in effect. Host Requirements The Linux host on which Sysbox runs must meet the following requirements: It must have one of the supported Linux distros . Systemd must be the system's process-manager (the default in the supported distros). Installing Sysbox NOTE : if you have a prior version of Sysbox already installed, please uninstall it first and then follow the installation instructions below. Download the latest Sysbox package from the release page. Verify that the checksum of the downloaded file fully matches the expected/published one. For example: $ shasum sysbox-ce_0.3.0-0.ubuntu-focal_amd64.deb 4850d18ed2af73f2819820cd8993f9cdc647cc79 sysbox-ce_0.3.0-0.ubuntu-focal_amd64.deb If Docker is running on the host, stop and remove all running Docker containers: $ docker rm $(docker ps -a -q) -f (if an error is returned, it simply indicates that no existing containers were found). This is necessary because the Sysbox installer may need to configure and restart Docker (to make Docker aware of Sysbox). It's possible to avoid the Docker restart; see Installing Sysbox w/o Docker restart below for more on this. Install the Sysbox package and follow the installer instructions: $ sudo apt-get install ./sysbox-ce_0.3.0-0.ubuntu-focal_amd64.deb -y Verify that Sysbox's Systemd units have been properly installed, and associated daemons are properly running: $ sudo systemctl status sysbox -n20 \u25cf sysbox.service - Sysbox container runtime Loaded: loaded (/lib/systemd/system/sysbox.service; enabled; vendor preset: enabled) Active: active (running) since Sat 2021-03-27 00:15:36 EDT; 20s ago Docs: https://github.com/nestybox/sysbox Main PID: 2305016 (sh) Tasks: 2 (limit: 9487) Memory: 792.0K CGroup: /system.slice/sysbox.service \u251c\u25002305016 /bin/sh -c /usr/bin/sysbox-runc --version && /usr/bin/sysbox-mgr --version && /usr/bin/sysbox-fs --version && /bin/sleep infinity \u2514\u25002305039 /bin/sleep infinity Mar 27 00:15:36 dev-vm1 systemd[1]: Started Sysbox container runtime. Mar 27 00:15:36 dev-vm1 sh[2305018]: sysbox-runc Mar 27 00:15:36 dev-vm1 sh[2305018]: edition: Community Edition (CE) Mar 27 00:15:36 dev-vm1 sh[2305018]: version: 0.3.0 Mar 27 00:15:36 dev-vm1 sh[2305018]: commit: df952e5276cb6e705e0be331e9a9fe88f372eab8 Mar 27 00:15:36 dev-vm1 sh[2305018]: built at: Sat Mar 27 01:34:12 UTC 2021 Mar 27 00:15:36 dev-vm1 sh[2305018]: built by: Rodny Molina Mar 27 00:15:36 dev-vm1 sh[2305018]: oci-specs: 1.0.2-dev Mar 27 00:15:36 dev-vm1 sh[2305024]: sysbox-mgr Mar 27 00:15:36 dev-vm1 sh[2305024]: edition: Community Edition (CE) Mar 27 00:15:36 dev-vm1 sh[2305024]: version: 0.3.0 Mar 27 00:15:36 dev-vm1 sh[2305024]: commit: 6ae5668e797ee1bb88fd5f5ae663873a87541ecb Mar 27 00:15:36 dev-vm1 sh[2305024]: built at: Sat Mar 27 01:34:41 UTC 2021 Mar 27 00:15:36 dev-vm1 sh[2305024]: built by: Rodny Molina Mar 27 00:15:36 dev-vm1 sh[2305031]: sysbox-fs Mar 27 00:15:36 dev-vm1 sh[2305031]: edition: Community Edition (CE) Mar 27 00:15:36 dev-vm1 sh[2305031]: version: 0.3.0 Mar 27 00:15:36 dev-vm1 sh[2305031]: commit: bb001b7fe2a0a234fe86ab20045677470239e248 Mar 27 00:15:36 dev-vm1 sh[2305031]: built at: Sat Mar 27 01:34:30 UTC 2021 Mar 27 00:15:36 dev-vm1 sh[2305031]: built by: Rodny Molina $ This indicates all Sysbox components are running properly.","title":"Prerequisites on the host system"},{"location":"prereq/#prerequisites-of-the-servermaster-node","text":"The Kubernetes cluster is running inside a docker container on an AMD64/x86_64 system. Sysbox Runtime , a open-source container runtime is used enable the docker container to esentially act as an VM enabling us to run a Kubernetes Cluster inside the container.","title":"Prerequisites of the Server/Master Node"},{"location":"prereq/#installing-docker-with-apt-get","text":"#!/bin/sh # https://docs.docker.com/engine/installation/linux/ubuntu/#install-using-the-repository sudo apt-get update && sudo apt-get install -y apt-transport-https ca-certificates curl software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-key fingerprint 0EBFCD88 | grep docker@docker.com || exit 1 sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" sudo apt-get update sudo apt-get install -y docker-ce sudo docker run --rm hello-world","title":"Installing docker with apt-get"},{"location":"prereq/#installing-using-the-latest-docker-release","text":"$ curl -fsSL https://get.docker.com/ | sh","title":"Installing using the latest Docker release"},{"location":"prereq/#finishing-installation","text":"Optional - add $USER to docker group (make sure to log out and back in after) $ sudo groupadd docker $ sudo usermod -aG docker $USER $ sudo systemctl restart docker Logout and login so that group changes are in effect.","title":"Finishing installation"},{"location":"prereq/#host-requirements","text":"The Linux host on which Sysbox runs must meet the following requirements: It must have one of the supported Linux distros . Systemd must be the system's process-manager (the default in the supported distros).","title":"Host Requirements"},{"location":"prereq/#installing-sysbox","text":"NOTE : if you have a prior version of Sysbox already installed, please uninstall it first and then follow the installation instructions below. Download the latest Sysbox package from the release page. Verify that the checksum of the downloaded file fully matches the expected/published one. For example: $ shasum sysbox-ce_0.3.0-0.ubuntu-focal_amd64.deb 4850d18ed2af73f2819820cd8993f9cdc647cc79 sysbox-ce_0.3.0-0.ubuntu-focal_amd64.deb If Docker is running on the host, stop and remove all running Docker containers: $ docker rm $(docker ps -a -q) -f (if an error is returned, it simply indicates that no existing containers were found). This is necessary because the Sysbox installer may need to configure and restart Docker (to make Docker aware of Sysbox). It's possible to avoid the Docker restart; see Installing Sysbox w/o Docker restart below for more on this. Install the Sysbox package and follow the installer instructions: $ sudo apt-get install ./sysbox-ce_0.3.0-0.ubuntu-focal_amd64.deb -y Verify that Sysbox's Systemd units have been properly installed, and associated daemons are properly running: $ sudo systemctl status sysbox -n20 \u25cf sysbox.service - Sysbox container runtime Loaded: loaded (/lib/systemd/system/sysbox.service; enabled; vendor preset: enabled) Active: active (running) since Sat 2021-03-27 00:15:36 EDT; 20s ago Docs: https://github.com/nestybox/sysbox Main PID: 2305016 (sh) Tasks: 2 (limit: 9487) Memory: 792.0K CGroup: /system.slice/sysbox.service \u251c\u25002305016 /bin/sh -c /usr/bin/sysbox-runc --version && /usr/bin/sysbox-mgr --version && /usr/bin/sysbox-fs --version && /bin/sleep infinity \u2514\u25002305039 /bin/sleep infinity Mar 27 00:15:36 dev-vm1 systemd[1]: Started Sysbox container runtime. Mar 27 00:15:36 dev-vm1 sh[2305018]: sysbox-runc Mar 27 00:15:36 dev-vm1 sh[2305018]: edition: Community Edition (CE) Mar 27 00:15:36 dev-vm1 sh[2305018]: version: 0.3.0 Mar 27 00:15:36 dev-vm1 sh[2305018]: commit: df952e5276cb6e705e0be331e9a9fe88f372eab8 Mar 27 00:15:36 dev-vm1 sh[2305018]: built at: Sat Mar 27 01:34:12 UTC 2021 Mar 27 00:15:36 dev-vm1 sh[2305018]: built by: Rodny Molina Mar 27 00:15:36 dev-vm1 sh[2305018]: oci-specs: 1.0.2-dev Mar 27 00:15:36 dev-vm1 sh[2305024]: sysbox-mgr Mar 27 00:15:36 dev-vm1 sh[2305024]: edition: Community Edition (CE) Mar 27 00:15:36 dev-vm1 sh[2305024]: version: 0.3.0 Mar 27 00:15:36 dev-vm1 sh[2305024]: commit: 6ae5668e797ee1bb88fd5f5ae663873a87541ecb Mar 27 00:15:36 dev-vm1 sh[2305024]: built at: Sat Mar 27 01:34:41 UTC 2021 Mar 27 00:15:36 dev-vm1 sh[2305024]: built by: Rodny Molina Mar 27 00:15:36 dev-vm1 sh[2305031]: sysbox-fs Mar 27 00:15:36 dev-vm1 sh[2305031]: edition: Community Edition (CE) Mar 27 00:15:36 dev-vm1 sh[2305031]: version: 0.3.0 Mar 27 00:15:36 dev-vm1 sh[2305031]: commit: bb001b7fe2a0a234fe86ab20045677470239e248 Mar 27 00:15:36 dev-vm1 sh[2305031]: built at: Sat Mar 27 01:34:30 UTC 2021 Mar 27 00:15:36 dev-vm1 sh[2305031]: built by: Rodny Molina $ This indicates all Sysbox components are running properly.","title":"Installing Sysbox"},{"location":"setup/","text":"Cluster Setup Create a Docker network $ sudo docker network create dkrnet Use the -v Docker option to mount the configuration file and log directory. You can start the container and bind to the dkrnet network as follows. Alternatively, you can replace dkrnet with host below to use Docker\u2019s host networking - if you only run a single container in your host. $ docker run -d -v /path/to/config-005.json:/etc/opt/evio/config.json -v /path/to/logs/005:/var/log/edge-vpnio/ --rm --privileged --name evio005-master --network dkrnet --runtime=sysbox-runc edgevpnio/evio-node:20.12.2 /sbin/init Exec into the Docker container $ docker exec -it evio005-master bash Install Docker and the prerequisites in the Docker Conatiner # curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg # echo \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | tee /etc/apt/sources.list.d/docker.list > /dev/null # apt update # apt-get install apt-transport-https ca-certificates curl gnupg lsb-release ssh vim docker-ce docker-ce-cli containerd.io -y Start the newly installed Docker service # systemctl start docker This project is using K3S , a Kubernetes distribution built for IoT & Edge computing. Install and start K3S with a single command after replacing the correct flannel interface. # curl -sfL https://get.k3s.io | sh -s - --docker --flannel-iface appbrXXXXXX --write-kubeconfig-mode 644 --write-kubeconfig $HOME/.kube/config If this has executed correctly, you should see some output similar to [INFO] Finding release for channel stable [INFO] Using v1.21.1+k3s1 as release [INFO] Downloading hash https://github.com/k3s-io/k3s/releases/download/v1.21.1+k3s1/sha256sum-amd64.txt [INFO] Downloading binary https://github.com/k3s-io/k3s/releases/download/v1.21.1+k3s1/k3s [INFO] Verifying binary download [INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Skipping /usr/local/bin/ctr symlink to k3s, command exists in PATH at /usr/bin/ctr [INFO] Creating killall script /usr/local/bin/k3s-killall.sh [INFO] Creating uninstall script /usr/local/bin/k3s-uninstall.sh [INFO] env: Creating environment file /etc/systemd/system/k3s.service.env [INFO] systemd: Creating service file /etc/systemd/system/k3s.service [INFO] systemd: Enabling k3s unit Created symlink /etc/systemd/system/multi-user.target.wants/k3s.service \u2192 /etc/systemd/system/k3s.service. [INFO] systemd: Starting k3s Find out the node token of the k3s cluster. # cat /var/lib/rancher/k3s/server/node-token K10973d9d8a2e95eb3fb473559cad8b414268gf266d0f000a045ecbbfe08fdf64d4::server:19e2c05131439792rb723801c54d2f78 Test if you are able to ping each of the nodes from the master node/docker container. # ping 10.10.100.1 # ping 10.10.100.2 # ping 10.10.100.3 On each of the following nodes, execute the below command to join the k3s cluster after replacing the K3S_URL , K3S_TOKEN and flannel-iface # curl -sfL https://get.k3s.io | K3S_URL=https://10.10.100.5:6443 K3S_TOKEN=K10973d9d8a2e95eb3fb473559cad8b414268gf266d0f000a045ecbbfe08fdf64d4::server:19e2c05131439792rb723801c54d2f78 sh -s - --docker --flannel-iface appbXXXXX After the Jetson nodes join the K3S cluster, label them as workers and respective node numbers which is used in nodeSelector attribute. # kubectl label node d2iedgeai node-role.kubernetes.io/worker=worker # kubectl label node d2iedgeai2 node-role.kubernetes.io/worker=worker # kubectl label node d2iedgeai3-desktop node-role.kubernetes.io/worker=worker # kubectl label nodes d2iedgeai nodenumber=one # kubectl label nodes d2iedgeai2 nodenumber=two # kubectl label nodes d2iedgeai3-desktop nodenumber=three Confirm all the nodes have joined the cluster and are in Ready status. # kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME d2iedgeai2 Ready worker 10h v1.21.1+k3s1 10.10.100.2 <none> Ubuntu 18.04.5 LTS 4.9.201+ docker://19.3.6 d2iedgeai3-desktop Ready worker 10h v1.21.1+k3s1 10.10.100.3 <none> Ubuntu 18.04.5 LTS 4.9.201+ docker://19.3.6 d2iedgeai Ready worker 10h v1.21.1+k3s1 10.10.100.1 <none> Ubuntu 18.04.5 LTS 4.9.201+ docker://19.3.6 5f632c8fe254 Ready control-plane,master 11h v1.21.1+k3s1 10.10.100.5 <none> Ubuntu 18.04.1 LTS 5.8.0-55-generic docker://20.10.7 The current cluster setup is made of 4 nodes in total. The main docker container runs on the server as the master/control plane, and the 3 Jetsons are worker nodes.","title":"Cluster setup"},{"location":"setup/#cluster-setup","text":"Create a Docker network $ sudo docker network create dkrnet Use the -v Docker option to mount the configuration file and log directory. You can start the container and bind to the dkrnet network as follows. Alternatively, you can replace dkrnet with host below to use Docker\u2019s host networking - if you only run a single container in your host. $ docker run -d -v /path/to/config-005.json:/etc/opt/evio/config.json -v /path/to/logs/005:/var/log/edge-vpnio/ --rm --privileged --name evio005-master --network dkrnet --runtime=sysbox-runc edgevpnio/evio-node:20.12.2 /sbin/init Exec into the Docker container $ docker exec -it evio005-master bash Install Docker and the prerequisites in the Docker Conatiner # curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg # echo \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | tee /etc/apt/sources.list.d/docker.list > /dev/null # apt update # apt-get install apt-transport-https ca-certificates curl gnupg lsb-release ssh vim docker-ce docker-ce-cli containerd.io -y Start the newly installed Docker service # systemctl start docker This project is using K3S , a Kubernetes distribution built for IoT & Edge computing. Install and start K3S with a single command after replacing the correct flannel interface. # curl -sfL https://get.k3s.io | sh -s - --docker --flannel-iface appbrXXXXXX --write-kubeconfig-mode 644 --write-kubeconfig $HOME/.kube/config If this has executed correctly, you should see some output similar to [INFO] Finding release for channel stable [INFO] Using v1.21.1+k3s1 as release [INFO] Downloading hash https://github.com/k3s-io/k3s/releases/download/v1.21.1+k3s1/sha256sum-amd64.txt [INFO] Downloading binary https://github.com/k3s-io/k3s/releases/download/v1.21.1+k3s1/k3s [INFO] Verifying binary download [INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Skipping /usr/local/bin/ctr symlink to k3s, command exists in PATH at /usr/bin/ctr [INFO] Creating killall script /usr/local/bin/k3s-killall.sh [INFO] Creating uninstall script /usr/local/bin/k3s-uninstall.sh [INFO] env: Creating environment file /etc/systemd/system/k3s.service.env [INFO] systemd: Creating service file /etc/systemd/system/k3s.service [INFO] systemd: Enabling k3s unit Created symlink /etc/systemd/system/multi-user.target.wants/k3s.service \u2192 /etc/systemd/system/k3s.service. [INFO] systemd: Starting k3s Find out the node token of the k3s cluster. # cat /var/lib/rancher/k3s/server/node-token K10973d9d8a2e95eb3fb473559cad8b414268gf266d0f000a045ecbbfe08fdf64d4::server:19e2c05131439792rb723801c54d2f78 Test if you are able to ping each of the nodes from the master node/docker container. # ping 10.10.100.1 # ping 10.10.100.2 # ping 10.10.100.3 On each of the following nodes, execute the below command to join the k3s cluster after replacing the K3S_URL , K3S_TOKEN and flannel-iface # curl -sfL https://get.k3s.io | K3S_URL=https://10.10.100.5:6443 K3S_TOKEN=K10973d9d8a2e95eb3fb473559cad8b414268gf266d0f000a045ecbbfe08fdf64d4::server:19e2c05131439792rb723801c54d2f78 sh -s - --docker --flannel-iface appbXXXXX After the Jetson nodes join the K3S cluster, label them as workers and respective node numbers which is used in nodeSelector attribute. # kubectl label node d2iedgeai node-role.kubernetes.io/worker=worker # kubectl label node d2iedgeai2 node-role.kubernetes.io/worker=worker # kubectl label node d2iedgeai3-desktop node-role.kubernetes.io/worker=worker # kubectl label nodes d2iedgeai nodenumber=one # kubectl label nodes d2iedgeai2 nodenumber=two # kubectl label nodes d2iedgeai3-desktop nodenumber=three Confirm all the nodes have joined the cluster and are in Ready status. # kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME d2iedgeai2 Ready worker 10h v1.21.1+k3s1 10.10.100.2 <none> Ubuntu 18.04.5 LTS 4.9.201+ docker://19.3.6 d2iedgeai3-desktop Ready worker 10h v1.21.1+k3s1 10.10.100.3 <none> Ubuntu 18.04.5 LTS 4.9.201+ docker://19.3.6 d2iedgeai Ready worker 10h v1.21.1+k3s1 10.10.100.1 <none> Ubuntu 18.04.5 LTS 4.9.201+ docker://19.3.6 5f632c8fe254 Ready control-plane,master 11h v1.21.1+k3s1 10.10.100.5 <none> Ubuntu 18.04.1 LTS 5.8.0-55-generic docker://20.10.7 The current cluster setup is made of 4 nodes in total. The main docker container runs on the server as the master/control plane, and the 3 Jetsons are worker nodes.","title":"Cluster Setup"},{"location":"testFlask/","text":"Testing the Flask application Once the kubectl apply command has been run, you should three deployments, services and pod started. # kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES d2iedgeai3-6f674fb4fd-8796p 1/1 Running 0 12h 10.42.1.30 d2iedgeai3-desktop <none> <none> d2iedgeai2-5db5448dd6-gn2xx 1/1 Running 0 12h 10.42.2.28 d2iedgeai2 <none> <none> d2iedgeai1-89d898b8d-6tv7j 1/1 Running 0 12h 10.42.3.31 d2iedgeai <none> <none> As you can see, the three pods are running on differnt nodes. To test the following application, select one of the pod IPs and run a curl . This should display the currently active pods which have IPs assigned. # curl -X POST 10.42.2.28:12000/ { \"10.42.1.30\": \"d2iedgeai3-6f674fb4fd-8796p\", \"10.42.2.28\": \"d2iedgeai2-5db5448dd6-gn2xx\", \"10.42.3.31\": \"d2iedgeai1-89d898b8d-6tv7j\" } Similarly for taking a random walk between the pods # curl 10.42.3.31:12000/start [ { \"node_ip\": \"10.42.3.31\", \"time\": \"06:38:58:880515\" }, { \"node_ip\": \"10.42.2.28\", \"time\": \"06:38:58:944868\" }, { \"node_ip\": \"10.42.1.30\", \"time\": \"06:38:58:965605\" } ]","title":"Testing the Flask application"},{"location":"testFlask/#testing-the-flask-application","text":"Once the kubectl apply command has been run, you should three deployments, services and pod started. # kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES d2iedgeai3-6f674fb4fd-8796p 1/1 Running 0 12h 10.42.1.30 d2iedgeai3-desktop <none> <none> d2iedgeai2-5db5448dd6-gn2xx 1/1 Running 0 12h 10.42.2.28 d2iedgeai2 <none> <none> d2iedgeai1-89d898b8d-6tv7j 1/1 Running 0 12h 10.42.3.31 d2iedgeai <none> <none> As you can see, the three pods are running on differnt nodes. To test the following application, select one of the pod IPs and run a curl . This should display the currently active pods which have IPs assigned. # curl -X POST 10.42.2.28:12000/ { \"10.42.1.30\": \"d2iedgeai3-6f674fb4fd-8796p\", \"10.42.2.28\": \"d2iedgeai2-5db5448dd6-gn2xx\", \"10.42.3.31\": \"d2iedgeai1-89d898b8d-6tv7j\" } Similarly for taking a random walk between the pods # curl 10.42.3.31:12000/start [ { \"node_ip\": \"10.42.3.31\", \"time\": \"06:38:58:880515\" }, { \"node_ip\": \"10.42.2.28\", \"time\": \"06:38:58:944868\" }, { \"node_ip\": \"10.42.1.30\", \"time\": \"06:38:58:965605\" } ]","title":"Testing the Flask application"},{"location":"topology/","text":"Node Topology","title":"Node Topology"},{"location":"topology/#node-topology","text":"","title":"Node Topology"},{"location":"withoutvpn/","text":"Running the cluster without EdgeVPN The Kubernetes cluster is running on crossbill (129.79.186.147) on port 6443. Steps to access the edge Kubernetes cluster Install a local command line version of kubectl. Skip this step if you have already installed it previously. This can be verified by running kubectl --version . Make sure you are connected to the IU Network/VPN. Export the config (provided by D2I) file for your local kubectl to use by running: export KUBECONFIG=~/exampleConfig.yaml Now the cluster can be accessed remotely from your local computer. $ kubectl get nodes NAME STATUS ROLES AGE VERSION crossbill Ready control-plane,master 2d22h v1.21.7+k3s1 d2iedgeai Ready worker 2d22h v1.21.7+k3s1 d2iedgeai3 Ready worker 2d22h v1.21.7+k3s1 d2iedgeai2 Ready worker 2d22h v1.21.7+k3s1 Configuring the cluster to use EdgeVPN Install the required pre-requistes mentioned in the previous webpages. Start the EdgeVPN docker container on the head node. Enable/start the Evio service on the nodes by running: sudo systemctl start evio Once the edgeVPN is running on the nodes, instaniate the kubernetes cluster on the head node. Once the kubernetes cluster is up and running on the head node, copy the cluster key and run the Kuberenets bash start script to join the head node from each of the edge devices. The cluster should be up and running. You can copy the kubeconfig file to use it for remote access.","title":"Cluster without EdgeVPN"},{"location":"withoutvpn/#running-the-cluster-without-edgevpn","text":"The Kubernetes cluster is running on crossbill (129.79.186.147) on port 6443.","title":"Running the cluster without EdgeVPN"},{"location":"withoutvpn/#steps-to-access-the-edge-kubernetes-cluster","text":"Install a local command line version of kubectl. Skip this step if you have already installed it previously. This can be verified by running kubectl --version . Make sure you are connected to the IU Network/VPN. Export the config (provided by D2I) file for your local kubectl to use by running: export KUBECONFIG=~/exampleConfig.yaml Now the cluster can be accessed remotely from your local computer. $ kubectl get nodes NAME STATUS ROLES AGE VERSION crossbill Ready control-plane,master 2d22h v1.21.7+k3s1 d2iedgeai Ready worker 2d22h v1.21.7+k3s1 d2iedgeai3 Ready worker 2d22h v1.21.7+k3s1 d2iedgeai2 Ready worker 2d22h v1.21.7+k3s1","title":"Steps to access the edge Kubernetes cluster"},{"location":"withoutvpn/#configuring-the-cluster-to-use-edgevpn","text":"Install the required pre-requistes mentioned in the previous webpages. Start the EdgeVPN docker container on the head node. Enable/start the Evio service on the nodes by running: sudo systemctl start evio Once the edgeVPN is running on the nodes, instaniate the kubernetes cluster on the head node. Once the kubernetes cluster is up and running on the head node, copy the cluster key and run the Kuberenets bash start script to join the head node from each of the edge devices. The cluster should be up and running. You can copy the kubeconfig file to use it for remote access.","title":"Configuring the cluster to use EdgeVPN"}]}